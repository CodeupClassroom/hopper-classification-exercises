{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:  Acquire data\n",
    "\n",
    "## Goals\n",
    "\n",
    "Data you wish to use in analysis will be stored in a variety of sources. In this lesson, we will review importing data from a csv and via mySQL, and we will also learn how to import data from our local clipboard, a google sheets document, and from an MS Excel file. \n",
    "We will then select one source to use as we continue through the rest of this module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Data Acquisition\n",
    "\n",
    "\n",
    "- `read_clipboard`: When you have data copied to your clipboard, you can use pandas to read it into a data frame with `pd.read_clipboard`. This can be useful for quickly transferring data to/from a spreadsheet.   \n",
    "- `read_excel`: This function can be used to create a data frame based on the contents of an Excel spreadsheet.  \n",
    "- `read_csv`: Read from a local csv, or from a the cloud (Google Sheets or AWS S3).    \n",
    "- `read_sql(sql_query, connection_url)`: Read data using a SQL query to a database. You must have the required drivers installed, and a specially formatted url string must be provided.\n",
    "    \n",
    "    ```\n",
    "    # To talk to a mysql database:\n",
    "    python -m pip install pymysql mysql-connector\n",
    "    # the connection url string:\n",
    "    mysql+pymysql://USER:PASSWORD@HOST/DATABASE_NAME\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: A Shared Google Sheet\n",
    "\n",
    "1. Get the shareable link url: https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/edit#gid=NUMBER\n",
    "\n",
    "2. Turn that into a CSV export URL: Replace `/edit` with `/export`; Add `format=csv` to the beginning of the query string. https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/export?format=csv&gid=NUMBER:\n",
    "\n",
    "3. Pass it to pd.read_csv, which can take a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "csv_export_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the S3 file is private, you will need your S3 configurations setup properly.\n",
    "df_s3 = pd.read_csv('https://s3.amazonaws.com/irs-form-990/index_2011.csv')\n",
    "df_s3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: SQL\n",
    "\n",
    "Create a dataframe from the passengers table in the mySQL database, titanic_db. \n",
    "\n",
    "!!!warning \"Database Credentials\"\n",
    "    It's a bad idea to store your database access credentials (i.e. your username and password) in plaintext in your source code. There are many different ways one could manage secrets like this, but a simple way is to store the values in a python file that is not included along with the rest of your source code. This is what we have done with the `env` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that we can reference later to acquire the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_titanic_data():\n",
    "    return pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store this function in a file named `acquire.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Your Data\n",
    "Because data acquisition can take time, it's a common practice to write the data locally to a `.csv` file. \n",
    "\n",
    "1. Do whatever you need to do to produce the dataframe that you need. \n",
    "    - For example `df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))`\n",
    "    - Or your dataframe cound include joins, multiple data sources, etc...\n",
    "    \n",
    "2. Now use `df.to_csv(\"titanic.csv\")` to write that dataframe to the file.\n",
    "3. Now that you've written the csv file, you can use it later in other parts of your pipeline!\n",
    "4. Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_titanic_data():\n",
    "    filename = \"titanic.csv\"\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        # read the SQL query into a dataframe\n",
    "        df = new_titanic_data()\n",
    "        \n",
    "        # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "        df.to_csv(filename)\n",
    "\n",
    "        # Return the dataframe to the calling code\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of these exercise is a jupyter notebook (`classification_exercises.ipynb`) and a `acquire.py` file. The notebook will contain all your work as you move through the exercises. The `acquire.py` file should contain the final functions that acquire the data into a pandas dataframe.\n",
    "\n",
    "1. Make a new repo called `classification-exercises` on both GitHub and within your `codeup-data-science` directory. This will be where you do your work for this module. \n",
    "\n",
    "2. Inside of your local `classification-exercises` repo, create a file named `.gitignore` and list the following file names and paths: `env.py`, `.DS_Store`, `.ipynb_checkpoints/`, `__pycache__`, `titanic.csv`, `iris.csv`, and `telco.csv`. Add and commit your `.gitignore` file before moving forward.\n",
    "    \n",
    "3. Now that you are 100% sure that your `.gitignore` file lists `env.py`, create or copy your `env.py` file inside of `classification-exercises`. Running `git status` should show that git is ignoring this file.\n",
    "\n",
    "4. In a jupyter notebook, `classification_exercises.ipynb`, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, `df_iris`, from this data.\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns (shape)\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables. Would you\n",
    "      recommend rescaling the data based on these statistics?\n",
    "\n",
    "\n",
    "5. Read the `Table1_CustDetails` table from the `Excel_Exercises.xlsx` file into a\n",
    "   dataframe named `df_excel`.\n",
    "\n",
    "    - assign the first 100 rows to a new dataframe, `df_excel_sample`\n",
    "    - print the number of rows of your original dataframe\n",
    "    - print the first 5 column names\n",
    "    - print the column names that have a data type of `object`\n",
    "    - compute the range for each of the numeric variables.\n",
    "\n",
    "\n",
    "6. Read the data from [this google sheet](https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing) into a dataframe, `df_google`\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables\n",
    "    - print the unique values for each of your categorical variables\n",
    "\n",
    "\n",
    "Make a new python module, `acquire.py` to hold the following data aquisition functions:\n",
    "\n",
    "7. Make a function named `get_titanic_data` that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "\n",
    "8. Make a function named `get_iris_data` that returns the data from the `iris_db` on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the `species_id`s. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "9. Make a function named `get_telco_data` that returns the data from the `telco_churn` database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "10. Once you've got your `get_titanic_data`, `get_iris_data`, and `get_telco_data` functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of `telco.csv`, `titanic.csv`, or `iris.csv`. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. \n",
    "\n",
    "__Be sure to add env.py, titanic.csv, iris.csv, and telco.csv to your .gitignore file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
